2018-09-19 14:09:15 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:09:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:20:00 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:20:00 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:20:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:20:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:20:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:20:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:20:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:20:00 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:20:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:20:00 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:20:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:20:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:20:00 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:20:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:20:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:20:01 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:20:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 20, 1, 49611),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 775606272,
 'memusage/startup': 775606272,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 20, 0, 911311)}
2018-09-19 14:20:01 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:21:48 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:21:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:21:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:21:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:21:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:21:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:21:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:21:48 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:21:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:21:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:21:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:21:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:21:48 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:21:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:21:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:21:49 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:21:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 21, 49, 47971),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 778891264,
 'memusage/startup': 778891264,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 21, 48, 906389)}
2018-09-19 14:21:49 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:23:06 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:23:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:23:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:23:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:23:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:23:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:23:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:23:06 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:23:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:23:06 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:23:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:23:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:23:06 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:23:06 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:23:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:23:07 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(response.body.decode("utf-8"))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd2 in position 15322: invalid continuation byte
2018-09-19 14:23:07 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:23:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 23, 7, 174560),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 784658432,
 'memusage/startup': 784658432,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 23, 6, 952658)}
2018-09-19 14:23:07 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:23:17 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:23:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:23:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:23:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:23:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:23:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:23:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:23:17 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:23:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:23:17 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:23:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:23:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:23:17 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:23:17 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:23:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:23:18 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(response.body.encode("utf-8"))
AttributeError: 'bytes' object has no attribute 'encode'
2018-09-19 14:23:18 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:23:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 23, 18, 150232),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 785387520,
 'memusage/startup': 785387520,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 23, 17, 928554)}
2018-09-19 14:23:18 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:25:35 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:25:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:25:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:25:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:25:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:25:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:25:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:25:35 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:25:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:25:35 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:25:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:25:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:25:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:25:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:25:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:25:35 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(response.body + "")
TypeError: can't concat str to bytes
2018-09-19 14:25:35 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:25:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 25, 35, 901925),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 787890176,
 'memusage/startup': 787890176,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 25, 35, 680661)}
2018-09-19 14:25:35 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:25:44 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:25:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:25:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:25:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:25:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:25:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:25:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:25:44 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:25:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:25:44 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:25:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:25:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:25:44 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:25:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:25:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:25:44 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:25:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 25, 44, 879054),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 788246528,
 'memusage/startup': 788246528,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 25, 44, 731986)}
2018-09-19 14:25:44 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:26:18 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:26:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:26:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:26:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:26:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:26:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:26:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:26:18 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:26:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:26:18 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:26:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:26:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:26:18 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:26:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:26:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:26:18 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(response.body.encode("utf-8"))
AttributeError: 'bytes' object has no attribute 'encode'
2018-09-19 14:26:18 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:26:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 26, 18, 494273),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 789344256,
 'memusage/startup': 789344256,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 26, 18, 272448)}
2018-09-19 14:26:18 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:29:28 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:29:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:29:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:29:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:29:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:29:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:29:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:29:28 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:29:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:29:28 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:29:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:29:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:29:28 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:29:28 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:29:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:29:28 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(bytes.decode(response.body))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd2 in position 15322: invalid continuation byte
2018-09-19 14:29:28 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:29:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 29, 28, 947792),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 790605824,
 'memusage/startup': 790605824,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 29, 28, 724226)}
2018-09-19 14:29:28 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:30:45 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:30:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:30:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:30:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:30:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:30:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:30:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:30:45 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:30:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:30:45 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:30:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:30:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:30:45 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:30:45 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:30:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:30:45 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:30:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 30, 45, 723067),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 791347200,
 'memusage/startup': 791347200,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 30, 45, 578480)}
2018-09-19 14:30:45 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:31:09 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:31:09 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:31:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:31:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:31:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:31:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:31:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:31:09 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:31:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:31:09 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:31:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:31:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:31:09 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:31:09 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:31:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:31:10 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(str(response.body, encoding="utf-8"))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd2 in position 15322: invalid continuation byte
2018-09-19 14:31:10 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:31:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 31, 10, 124942),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 791941120,
 'memusage/startup': 791941120,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 31, 9, 902684)}
2018-09-19 14:31:10 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:32:38 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:32:38 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:32:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:32:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:32:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:32:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:32:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:32:38 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:32:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:32:38 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:32:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:32:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:32:38 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:32:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:32:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:32:38 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(response.body.decode('utf-8'))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd2 in position 15322: invalid continuation byte
2018-09-19 14:32:39 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:32:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 32, 39, 60373),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 792481792,
 'memusage/startup': 792481792,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 32, 38, 837181)}
2018-09-19 14:32:39 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:32:54 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:32:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:32:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:32:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:32:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:32:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:32:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:32:55 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:32:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:32:55 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:32:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:32:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:32:55 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:32:55 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:32:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:32:55 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(type(response.body.decode('utf-8')))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd2 in position 15322: invalid continuation byte
2018-09-19 14:32:55 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:32:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 32, 55, 241465),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 792481792,
 'memusage/startup': 792481792,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 32, 55, 19540)}
2018-09-19 14:32:55 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:33:12 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:33:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:33:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:33:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:33:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:33:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:33:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:33:12 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:33:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:33:12 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:33:12 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:33:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:33:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:33:12 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:33:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 33, 12, 246548),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 792690688,
 'memusage/startup': 792690688,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 33, 12, 117765)}
2018-09-19 14:33:12 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:33:58 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:33:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:33:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:33:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:33:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:33:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:33:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:33:58 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:33:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:33:58 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:33:58 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:33:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:33:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:33:59 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(type(response.body.decode('GBK')))
UnicodeDecodeError: 'gbk' codec can't decode byte 0xa4 in position 119742: illegal multibyte sequence
2018-09-19 14:33:59 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:33:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 33, 59, 116838),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 812457984,
 'memusage/startup': 812457984,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 33, 58, 890537)}
2018-09-19 14:33:59 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:34:11 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:34:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:34:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:34:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:34:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:34:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:34:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:34:11 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:34:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:34:11 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:34:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:34:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:34:11 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:34:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:34:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:34:11 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 15, in parse
    print(response.body.decode('GBK'))
UnicodeDecodeError: 'gbk' codec can't decode byte 0xa4 in position 119742: illegal multibyte sequence
2018-09-19 14:34:12 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:34:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 34, 12, 74671),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 812670976,
 'memusage/startup': 812670976,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 34, 11, 847846)}
2018-09-19 14:34:12 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:36:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:36:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:36:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:36:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:36:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:36:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:36:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:36:31 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:36:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:36:31 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:36:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:36:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:36:31 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:36:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:36:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
2018-09-19 14:36:32 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 16, in parse
    print(response.body.decode('GBK'))
UnicodeDecodeError: 'gbk' codec can't decode byte 0xac in position 72: illegal multibyte sequence
2018-09-19 14:36:32 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:36:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 867,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 77415,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 36, 32, 148864),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 834334720,
 'memusage/startup': 834334720,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 36, 31, 927341)}
2018-09-19 14:36:32 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:36:42 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:36:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:36:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:36:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:36:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:36:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:36:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:36:42 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:36:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:36:42 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:36:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:36:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:36:42 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:36:42 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:36:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
2018-09-19 14:36:42 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:36:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 867,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 77415,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 36, 42, 239597),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 834334720,
 'memusage/startup': 834334720,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 36, 42, 108689)}
2018-09-19 14:36:42 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:37:03 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:37:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:37:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:37:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:37:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:37:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:37:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:37:03 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:37:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:37:03 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:37:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:37:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:37:03 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:37:03 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:37:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
2018-09-19 14:37:03 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:37:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 867,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 77415,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 37, 3, 697148),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 835272704,
 'memusage/startup': 835272704,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 37, 3, 568285)}
2018-09-19 14:37:03 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:37:38 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:37:38 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:37:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:37:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:37:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:37:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:37:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:37:38 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:37:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:37:38 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:37:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:37:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:37:38 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:37:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:37:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:37:38 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:37:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 37, 38, 472115),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 835502080,
 'memusage/startup': 835502080,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 37, 38, 332531)}
2018-09-19 14:37:38 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:39:04 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:39:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:39:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:39:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:39:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:39:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:39:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:39:04 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:39:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:39:04 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:39:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:39:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:39:04 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:39:04 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:39:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:39:04 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 16, in parse
    print(response.body.decode('gbk'))
UnicodeDecodeError: 'gbk' codec can't decode byte 0xa4 in position 119742: illegal multibyte sequence
2018-09-19 14:39:04 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:39:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 39, 4, 437617),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 838021120,
 'memusage/startup': 838021120,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 39, 4, 211346)}
2018-09-19 14:39:04 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:41:15 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:41:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:41:15 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:41:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:41:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:41:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:41:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:41:15 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:41:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:41:15 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:41:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:41:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:41:15 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:41:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 16, in parse
    print(response.body.encode('gbk'))
AttributeError: 'bytes' object has no attribute 'encode'
2018-09-19 14:41:15 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:41:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 41, 15, 702745),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 861413376,
 'memusage/startup': 861413376,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 41, 15, 481259)}
2018-09-19 14:41:15 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:45:41 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:45:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:45:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:45:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:45:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:45:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:45:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:45:41 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:45:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:45:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:45:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:45:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:45:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:45:41 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:45:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:45:41 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 16, in parse
    print(response.body.encode('gb2312'))
AttributeError: 'bytes' object has no attribute 'encode'
2018-09-19 14:45:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:45:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 45, 41, 826146),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 861671424,
 'memusage/startup': 861671424,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 45, 41, 604623)}
2018-09-19 14:45:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:45:56 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:45:56 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:45:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:45:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:45:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:45:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:45:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:45:56 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:45:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:45:56 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:45:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:45:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:45:56 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:45:56 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:45:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
2018-09-19 14:45:56 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.htm> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 16, in parse
    print(response.body.encode('GB2312'))
AttributeError: 'bytes' object has no attribute 'encode'
2018-09-19 14:45:57 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:45:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 864,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 45, 57, 15264),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 861777920,
 'memusage/startup': 861777920,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 45, 56, 794801)}
2018-09-19 14:45:57 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 14:46:13 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 14:46:13 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 14:46:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 14:46:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 14:46:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 14:46:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 14:46:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 14:46:13 [scrapy.core.engine] INFO: Spider opened
2018-09-19 14:46:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 14:46:13 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 14:46:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:46:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:46:13 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:46:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 14:46:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
2018-09-19 14:46:13 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 16, in parse
    print(response.body.encode('GB2312'))
AttributeError: 'bytes' object has no attribute 'encode'
2018-09-19 14:46:13 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 14:46:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 867,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 77415,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 6, 46, 13, 954538),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 862081024,
 'memusage/startup': 862081024,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 6, 46, 13, 735237)}
2018-09-19 14:46:13 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:01:17 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:01:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:01:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:01:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:01:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:01:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:01:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:01:17 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:01:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:01:17 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:01:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:17 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:17 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
2018-09-19 15:01:17 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 16, in parse
    print(response.body.encode('utf-8'))
AttributeError: 'bytes' object has no attribute 'encode'
2018-09-19 15:01:17 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:01:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 867,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 77415,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 1, 17, 751288),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 1069105152,
 'memusage/startup': 1069105152,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 1, 17, 534990)}
2018-09-19 15:01:17 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:01:30 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:01:30 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:01:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:01:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:01:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:01:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:01:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:01:30 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:01:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:01:30 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:01:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:30 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:30 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/xici.html> (referer: None)
2018-09-19 15:01:30 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:01:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 867,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 77415,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 1, 30, 440277),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1069105152,
 'memusage/startup': 1069105152,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 1, 30, 308844)}
2018-09-19 15:01:30 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:01:51 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:01:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:01:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:01:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:01:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:01:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:01:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:01:51 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:01:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:01:51 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:01:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:51 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:01:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:01:51 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 16, in parse
    print(response.body.decode('utf-8'))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd2 in position 15322: invalid continuation byte
2018-09-19 15:01:51 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:01:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 1, 51, 304783),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 1069256704,
 'memusage/startup': 1069256704,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 1, 51, 83040)}
2018-09-19 15:01:51 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:02:18 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:02:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:02:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:02:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:02:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:02:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:02:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:02:18 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:02:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:02:18 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:02:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:02:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:02:18 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:02:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:02:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:02:18 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:02:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 2, 18, 733640),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1069817856,
 'memusage/startup': 1069817856,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 2, 18, 594116)}
2018-09-19 15:02:18 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:09:16 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:09:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:09:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:09:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:09:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:09:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:09:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:09:16 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:09:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:09:16 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:09:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:09:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:09:16 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:09:16 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:09:16 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:09:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 9, 16, 970376),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1070563328,
 'memusage/startup': 1070563328,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 9, 16, 816527)}
2018-09-19 15:09:16 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:09:37 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:09:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:09:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:09:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:09:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:09:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:09:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:09:37 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:09:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:09:37 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:09:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:09:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:09:37 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:09:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:09:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:09:37 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:09:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 9, 37, 606906),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1071022080,
 'memusage/startup': 1071022080,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 9, 37, 469170)}
2018-09-19 15:09:37 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:10:25 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:10:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:10:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:10:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:10:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:10:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:10:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:10:25 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:10:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:10:25 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:10:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:10:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:10:25 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:10:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:10:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:10:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:10:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 10, 25, 366763),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1072009216,
 'memusage/startup': 1072009216,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 10, 25, 224448)}
2018-09-19 15:10:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:10:50 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:10:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:10:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:10:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:10:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:10:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:10:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:10:50 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:10:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:10:50 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:10:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:10:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:10:50 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:10:50 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:10:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:10:50 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:10:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 10, 50, 758853),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1072562176,
 'memusage/startup': 1072562176,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 10, 50, 609060)}
2018-09-19 15:10:50 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:11:06 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:11:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:11:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:11:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:11:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:11:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:11:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:11:07 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:11:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:11:07 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:11:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:07 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:07 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:11:07 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:11:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 11, 7, 175092),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1072656384,
 'memusage/startup': 1072656384,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 11, 7, 20949)}
2018-09-19 15:11:07 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:11:41 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:11:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:11:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:11:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:11:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:11:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:11:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:11:41 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:11:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:11:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:11:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:41 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:11:41 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/Scrapy/example/example/spiders/tm_goods.py", line 17, in parse
    divs = response.xpath("//div[@id='J_ItemList']/div[@class='product']")[0].extract()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/parsel/selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-09-19 15:11:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:11:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 11, 41, 927216),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 1072783360,
 'memusage/startup': 1072783360,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 11, 41, 686991)}
2018-09-19 15:11:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:11:53 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:11:53 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:11:53 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:11:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:11:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:11:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:11:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:11:53 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:11:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:11:53 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:11:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:53 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:53 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:11:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:11:53 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:11:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 11, 53, 843001),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1072902144,
 'memusage/startup': 1072902144,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 11, 53, 700513)}
2018-09-19 15:11:53 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:12:45 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:12:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:12:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:12:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:12:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:12:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:12:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:12:45 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:12:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:12:45 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:12:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:12:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:12:45 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:12:45 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:12:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:12:45 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:12:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 12, 45, 635416),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1073143808,
 'memusage/startup': 1073143808,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 12, 45, 492492)}
2018-09-19 15:12:45 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:19:26 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:19:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:19:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:19:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:19:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:19:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:19:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:19:26 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:19:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:19:26 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:19:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:19:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:19:26 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:19:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:19:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:19:26 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:19:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 19, 26, 740843),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 19, 26, 594740)}
2018-09-19 15:19:26 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:22:37 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:22:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:22:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:22:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:22:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:22:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:22:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:22:37 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:22:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:22:37 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:22:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:22:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:22:37 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:22:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:22:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:22:37 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:22:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 22, 37, 810480),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 22, 37, 657028)}
2018-09-19 15:22:37 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:25:48 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:25:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:25:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:25:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:25:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:25:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:25:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:25:48 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:25:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:25:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:25:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:25:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:25:48 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:25:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:25:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:25:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:25:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 25, 48, 912147),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 25, 48, 759414)}
2018-09-19 15:25:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:27:05 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:27:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:27:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:27:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:27:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:27:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:27:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:27:05 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:27:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:27:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:27:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:27:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:27:05 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:27:05 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:27:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:27:05 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:27:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 27, 5, 865128),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 27, 5, 710522)}
2018-09-19 15:27:05 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 15:27:50 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 15:27:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 15:27:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 15:27:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 15:27:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 15:27:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 15:27:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 15:27:50 [scrapy.core.engine] INFO: Spider opened
2018-09-19 15:27:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 15:27:50 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 15:27:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:27:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:27:50 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:27:50 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 15:27:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 15:27:51 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 15:27:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 865,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 349969,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 7, 27, 51, 9521),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 19, 7, 27, 50, 851379)}
2018-09-19 15:27:51 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 17:15:00 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 17:15:00 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 17:15:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 17:15:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 17:15:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 17:15:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 17:15:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 17:15:00 [scrapy.core.engine] INFO: Spider opened
2018-09-19 17:15:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 17:15:00 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:15:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 17:15:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://detail.tmall.com/robots.txt> (referer: None)
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=10152256872&skuId=78140728882&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564857606526&skuId=3572946977984&standard=1&user_id=2972174697&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=44198874506&skuId=3263893868155&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=17327350365&skuId=3916383698416&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=553273646296&skuId=3396571852717&standard=1&user_id=2616970884&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523307965803&skuId=3742872456901&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39916555226&skuId=3520615749309&standard=1&user_id=1699764759&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564100865607&skuId=3899592779357&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569647996683&skuId=3916326182535&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559258759150&skuId=3647414346979&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45716529622&skuId=3657827257584&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39235505054&skuId=3246644221942&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545020495706&skuId=3814932525943&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540210701968&skuId=3917892555229&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=36299592403&skuId=59473355147&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959928102&skuId=3812905236795&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570597116831&skuId=3851514038575&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902338584&skuId=3789050113843&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540346037673&skuId=3796809784729&standard=1&user_id=2133729733&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523160102571&skuId=3909670414447&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523242229702&skuId=3910224794399&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=566803058725&skuId=3603270917229&user_id=713805252&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545194381711&skuId=3451976174898&standard=1&user_id=3089116113&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221612873&skuId=3811793005884&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570262192129&skuId=3676994888459&standard=1&user_id=2357345202&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569120030736&skuId=3730782856965&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902686118&skuId=3773509711223&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538811534073&skuId=3916320122278&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545775752075&skuId=3741617055871&standard=1&user_id=3162810140&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564063940519&skuId=3943960855846&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=565399873867&skuId=3753159932463&standard=1&user_id=925588507&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564240962668&skuId=3753511052366&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540015263833&skuId=3929532822134&standard=1&user_id=2578636029&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45603735926&skuId=3850910643047&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569728176391&skuId=3831626550518&standard=1&user_id=2202597322&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527893326581&skuId=3195559274344&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564570934197&skuId=3732859846129&standard=1&user_id=1124540496&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=12620021281&skuId=67511695221&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559863950838&skuId=3761114936423&standard=1&user_id=737581011&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564482793462&skuId=3566498168740&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538776838968&skuId=3748932472633&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=561271454031&skuId=3713328932535&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=560178885692&skuId=3882569963923&standard=1&user_id=742795908&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554824935356&skuId=3584276187036&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45053862121&skuId=3684716254711&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570451573502&skuId=3755546720658&user_id=3817430018&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569129768864&skuId=3896327478114&standard=1&user_id=894127710&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=20351803834&skuId=3916397510844&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=25356196399&skuId=3804175157430&standard=1&user_id=695023374&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=550679604048&skuId=3950518295650&standard=1&user_id=3295616803&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=562830632869&skuId=3542491737156&user_id=2362132778&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221112652&skuId=3770833343471&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=536344697361&skuId=3421408557705&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=521322201495&skuId=3282961913302&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554738813806&skuId=3680965896205&standard=1&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959480906&skuId=3922173019832&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45669350384&skuId=3445712899582&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527922188250&skuId=3237788828996&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=40000831870&skuId=3911785195184&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:00 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=15354239784&skuId=3757349821066&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:01 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 17:15:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 63,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 60,
 'downloader/request_bytes': 1090,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 350663,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 9, 15, 1, 184678),
 'log_count/DEBUG': 66,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'request_depth_max': 1,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 61,
 'scheduler/dequeued/memory': 61,
 'scheduler/enqueued': 61,
 'scheduler/enqueued/memory': 61,
 'start_time': datetime.datetime(2018, 9, 19, 9, 15, 0, 647347)}
2018-09-19 17:15:01 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 17:15:40 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 17:15:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 17:15:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 17:15:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 17:15:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 17:15:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 17:15:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 17:15:40 [scrapy.core.engine] INFO: Spider opened
2018-09-19 17:15:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 17:15:40 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 17:15:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:15:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:15:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:15:40 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:15:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 17:15:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://detail.tmall.com/robots.txt> (referer: None)
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=10152256872&skuId=78140728882&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523307965803&skuId=3742872456901&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39916555226&skuId=3520615749309&standard=1&user_id=1699764759&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564100865607&skuId=3899592779357&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569647996683&skuId=3916326182535&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559258759150&skuId=3647414346979&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45716529622&skuId=3657827257584&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39235505054&skuId=3246644221942&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545020495706&skuId=3814932525943&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540210701968&skuId=3917892555229&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=36299592403&skuId=59473355147&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554738813806&skuId=3680965896205&standard=1&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959480906&skuId=3922173019832&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45669350384&skuId=3445712899582&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527922188250&skuId=3237788828996&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959928102&skuId=3812905236795&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570597116831&skuId=3851514038575&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902338584&skuId=3789050113843&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540346037673&skuId=3796809784729&standard=1&user_id=2133729733&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523160102571&skuId=3909670414447&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523242229702&skuId=3910224794399&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=566803058725&skuId=3603270917229&user_id=713805252&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545194381711&skuId=3451976174898&standard=1&user_id=3089116113&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221612873&skuId=3811793005884&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570262192129&skuId=3676994888459&standard=1&user_id=2357345202&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569120030736&skuId=3730782856965&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902686118&skuId=3773509711223&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538811534073&skuId=3916320122278&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545775752075&skuId=3741617055871&standard=1&user_id=3162810140&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564063940519&skuId=3943960855846&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=565399873867&skuId=3753159932463&standard=1&user_id=925588507&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564240962668&skuId=3753511052366&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540015263833&skuId=3929532822134&standard=1&user_id=2578636029&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45603735926&skuId=3850910643047&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569728176391&skuId=3831626550518&standard=1&user_id=2202597322&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527893326581&skuId=3195559274344&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564570934197&skuId=3732859846129&standard=1&user_id=1124540496&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=12620021281&skuId=67511695221&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559863950838&skuId=3761114936423&standard=1&user_id=737581011&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564482793462&skuId=3566498168740&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538776838968&skuId=3748932472633&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=561271454031&skuId=3713328932535&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=560178885692&skuId=3882569963923&standard=1&user_id=742795908&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554824935356&skuId=3584276187036&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45053862121&skuId=3684716254711&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570451573502&skuId=3755546720658&user_id=3817430018&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569129768864&skuId=3896327478114&standard=1&user_id=894127710&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=20351803834&skuId=3916397510844&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=25356196399&skuId=3804175157430&standard=1&user_id=695023374&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=550679604048&skuId=3950518295650&standard=1&user_id=3295616803&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=562830632869&skuId=3542491737156&user_id=2362132778&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221112652&skuId=3770833343471&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=536344697361&skuId=3421408557705&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=521322201495&skuId=3282961913302&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564857606526&skuId=3572946977984&standard=1&user_id=2972174697&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=44198874506&skuId=3263893868155&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=17327350365&skuId=3916383698416&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=553273646296&skuId=3396571852717&standard=1&user_id=2616970884&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=40000831870&skuId=3911785195184&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=15354239784&skuId=3757349821066&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:15:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 17:15:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 63,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 60,
 'downloader/request_bytes': 1090,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 350664,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 9, 15, 41, 487009),
 'log_count/DEBUG': 66,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'request_depth_max': 1,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 61,
 'scheduler/dequeued/memory': 61,
 'scheduler/enqueued': 61,
 'scheduler/enqueued/memory': 61,
 'start_time': datetime.datetime(2018, 9, 19, 9, 15, 40, 721001)}
2018-09-19 17:15:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 17:16:46 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 17:16:46 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 17:16:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 17:16:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 17:16:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 17:16:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 17:16:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 17:16:46 [scrapy.core.engine] INFO: Spider opened
2018-09-19 17:16:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 17:16:46 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:16:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 17:16:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://detail.tmall.com/robots.txt> (referer: None)
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=10152256872&skuId=78140728882&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=553273646296&skuId=3396571852717&standard=1&user_id=2616970884&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523307965803&skuId=3742872456901&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39916555226&skuId=3520615749309&standard=1&user_id=1699764759&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564100865607&skuId=3899592779357&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569647996683&skuId=3916326182535&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559258759150&skuId=3647414346979&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45716529622&skuId=3657827257584&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39235505054&skuId=3246644221942&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545020495706&skuId=3814932525943&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540210701968&skuId=3917892555229&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=36299592403&skuId=59473355147&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554738813806&skuId=3680965896205&standard=1&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959480906&skuId=3922173019832&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45669350384&skuId=3445712899582&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959928102&skuId=3812905236795&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570597116831&skuId=3851514038575&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902338584&skuId=3789050113843&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540346037673&skuId=3796809784729&standard=1&user_id=2133729733&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523160102571&skuId=3909670414447&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523242229702&skuId=3910224794399&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=566803058725&skuId=3603270917229&user_id=713805252&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545194381711&skuId=3451976174898&standard=1&user_id=3089116113&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221612873&skuId=3811793005884&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570262192129&skuId=3676994888459&standard=1&user_id=2357345202&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569120030736&skuId=3730782856965&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902686118&skuId=3773509711223&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538811534073&skuId=3916320122278&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545775752075&skuId=3741617055871&standard=1&user_id=3162810140&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564063940519&skuId=3943960855846&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=565399873867&skuId=3753159932463&standard=1&user_id=925588507&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564240962668&skuId=3753511052366&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540015263833&skuId=3929532822134&standard=1&user_id=2578636029&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45603735926&skuId=3850910643047&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569728176391&skuId=3831626550518&standard=1&user_id=2202597322&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527893326581&skuId=3195559274344&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564570934197&skuId=3732859846129&standard=1&user_id=1124540496&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=12620021281&skuId=67511695221&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559863950838&skuId=3761114936423&standard=1&user_id=737581011&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564482793462&skuId=3566498168740&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538776838968&skuId=3748932472633&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=561271454031&skuId=3713328932535&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=560178885692&skuId=3882569963923&standard=1&user_id=742795908&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554824935356&skuId=3584276187036&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45053862121&skuId=3684716254711&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570451573502&skuId=3755546720658&user_id=3817430018&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569129768864&skuId=3896327478114&standard=1&user_id=894127710&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:46 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=20351803834&skuId=3916397510844&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=25356196399&skuId=3804175157430&standard=1&user_id=695023374&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=550679604048&skuId=3950518295650&standard=1&user_id=3295616803&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=562830632869&skuId=3542491737156&user_id=2362132778&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221112652&skuId=3770833343471&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=536344697361&skuId=3421408557705&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=521322201495&skuId=3282961913302&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564857606526&skuId=3572946977984&standard=1&user_id=2972174697&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=44198874506&skuId=3263893868155&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=17327350365&skuId=3916383698416&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527922188250&skuId=3237788828996&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=40000831870&skuId=3911785195184&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=15354239784&skuId=3757349821066&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:16:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 17:16:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 63,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 60,
 'downloader/request_bytes': 1090,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 350664,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 9, 16, 47, 209816),
 'log_count/DEBUG': 66,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'request_depth_max': 1,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 61,
 'scheduler/dequeued/memory': 61,
 'scheduler/enqueued': 61,
 'scheduler/enqueued/memory': 61,
 'start_time': datetime.datetime(2018, 9, 19, 9, 16, 46, 132689)}
2018-09-19 17:16:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 17:17:12 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 17:17:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 17:17:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 17:17:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 17:17:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 17:17:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 17:17:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 17:17:12 [scrapy.core.engine] INFO: Spider opened
2018-09-19 17:17:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 17:17:12 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 17:17:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:17:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:17:12 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:17:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:17:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 17:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://detail.tmall.com/robots.txt> (referer: None)
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=10152256872&skuId=78140728882&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564857606526&skuId=3572946977984&standard=1&user_id=2972174697&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=44198874506&skuId=3263893868155&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=17327350365&skuId=3916383698416&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=553273646296&skuId=3396571852717&standard=1&user_id=2616970884&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523307965803&skuId=3742872456901&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39916555226&skuId=3520615749309&standard=1&user_id=1699764759&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564100865607&skuId=3899592779357&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569647996683&skuId=3916326182535&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559258759150&skuId=3647414346979&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45716529622&skuId=3657827257584&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39235505054&skuId=3246644221942&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545020495706&skuId=3814932525943&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540210701968&skuId=3917892555229&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=36299592403&skuId=59473355147&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959928102&skuId=3812905236795&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570597116831&skuId=3851514038575&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902338584&skuId=3789050113843&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540346037673&skuId=3796809784729&standard=1&user_id=2133729733&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523160102571&skuId=3909670414447&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523242229702&skuId=3910224794399&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=566803058725&skuId=3603270917229&user_id=713805252&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545194381711&skuId=3451976174898&standard=1&user_id=3089116113&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221612873&skuId=3811793005884&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570262192129&skuId=3676994888459&standard=1&user_id=2357345202&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569120030736&skuId=3730782856965&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902686118&skuId=3773509711223&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538811534073&skuId=3916320122278&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545775752075&skuId=3741617055871&standard=1&user_id=3162810140&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564063940519&skuId=3943960855846&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=565399873867&skuId=3753159932463&standard=1&user_id=925588507&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564240962668&skuId=3753511052366&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540015263833&skuId=3929532822134&standard=1&user_id=2578636029&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45603735926&skuId=3850910643047&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569728176391&skuId=3831626550518&standard=1&user_id=2202597322&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527893326581&skuId=3195559274344&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564570934197&skuId=3732859846129&standard=1&user_id=1124540496&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=12620021281&skuId=67511695221&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559863950838&skuId=3761114936423&standard=1&user_id=737581011&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564482793462&skuId=3566498168740&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538776838968&skuId=3748932472633&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=561271454031&skuId=3713328932535&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=560178885692&skuId=3882569963923&standard=1&user_id=742795908&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554824935356&skuId=3584276187036&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45053862121&skuId=3684716254711&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570451573502&skuId=3755546720658&user_id=3817430018&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569129768864&skuId=3896327478114&standard=1&user_id=894127710&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=20351803834&skuId=3916397510844&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=25356196399&skuId=3804175157430&standard=1&user_id=695023374&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=550679604048&skuId=3950518295650&standard=1&user_id=3295616803&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=562830632869&skuId=3542491737156&user_id=2362132778&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221112652&skuId=3770833343471&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=536344697361&skuId=3421408557705&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=521322201495&skuId=3282961913302&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554738813806&skuId=3680965896205&standard=1&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959480906&skuId=3922173019832&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45669350384&skuId=3445712899582&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527922188250&skuId=3237788828996&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=40000831870&skuId=3911785195184&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=15354239784&skuId=3757349821066&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:17:14 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 17:17:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 63,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 60,
 'downloader/request_bytes': 1090,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 350660,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 9, 17, 14, 400578),
 'log_count/DEBUG': 66,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'request_depth_max': 1,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 61,
 'scheduler/dequeued/memory': 61,
 'scheduler/enqueued': 61,
 'scheduler/enqueued/memory': 61,
 'start_time': datetime.datetime(2018, 9, 19, 9, 17, 12, 530287)}
2018-09-19 17:17:14 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 17:18:43 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 17:18:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 17:18:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 17:18:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 17:18:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 17:18:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 17:18:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 17:18:43 [scrapy.core.engine] INFO: Spider opened
2018-09-19 17:18:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 17:18:43 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 17:18:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:18:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:18:43 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:18:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 17:18:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://detail.tmall.com/robots.txt> (referer: None)
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=10152256872&skuId=78140728882&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569647996683&skuId=3916326182535&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559258759150&skuId=3647414346979&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45716529622&skuId=3657827257584&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39235505054&skuId=3246644221942&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545020495706&skuId=3814932525943&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540210701968&skuId=3917892555229&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=36299592403&skuId=59473355147&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554738813806&skuId=3680965896205&standard=1&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959480906&skuId=3922173019832&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45669350384&skuId=3445712899582&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527922188250&skuId=3237788828996&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=40000831870&skuId=3911785195184&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=15354239784&skuId=3757349821066&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=550679604048&skuId=3950518295650&standard=1&user_id=3295616803&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959928102&skuId=3812905236795&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570597116831&skuId=3851514038575&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902338584&skuId=3789050113843&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540346037673&skuId=3796809784729&standard=1&user_id=2133729733&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523160102571&skuId=3909670414447&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523242229702&skuId=3910224794399&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=566803058725&skuId=3603270917229&user_id=713805252&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545194381711&skuId=3451976174898&standard=1&user_id=3089116113&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221612873&skuId=3811793005884&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570262192129&skuId=3676994888459&standard=1&user_id=2357345202&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569120030736&skuId=3730782856965&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902686118&skuId=3773509711223&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538811534073&skuId=3916320122278&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545775752075&skuId=3741617055871&standard=1&user_id=3162810140&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564063940519&skuId=3943960855846&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=565399873867&skuId=3753159932463&standard=1&user_id=925588507&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564240962668&skuId=3753511052366&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540015263833&skuId=3929532822134&standard=1&user_id=2578636029&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45603735926&skuId=3850910643047&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569728176391&skuId=3831626550518&standard=1&user_id=2202597322&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527893326581&skuId=3195559274344&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564570934197&skuId=3732859846129&standard=1&user_id=1124540496&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=12620021281&skuId=67511695221&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559863950838&skuId=3761114936423&standard=1&user_id=737581011&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564482793462&skuId=3566498168740&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538776838968&skuId=3748932472633&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=561271454031&skuId=3713328932535&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=560178885692&skuId=3882569963923&standard=1&user_id=742795908&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554824935356&skuId=3584276187036&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45053862121&skuId=3684716254711&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570451573502&skuId=3755546720658&user_id=3817430018&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569129768864&skuId=3896327478114&standard=1&user_id=894127710&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=20351803834&skuId=3916397510844&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=25356196399&skuId=3804175157430&standard=1&user_id=695023374&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=562830632869&skuId=3542491737156&user_id=2362132778&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221112652&skuId=3770833343471&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=536344697361&skuId=3421408557705&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=521322201495&skuId=3282961913302&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564857606526&skuId=3572946977984&standard=1&user_id=2972174697&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=44198874506&skuId=3263893868155&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=17327350365&skuId=3916383698416&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=553273646296&skuId=3396571852717&standard=1&user_id=2616970884&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523307965803&skuId=3742872456901&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39916555226&skuId=3520615749309&standard=1&user_id=1699764759&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564100865607&skuId=3899592779357&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:18:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 17:18:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 63,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 60,
 'downloader/request_bytes': 1090,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 350664,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 9, 18, 47, 235419),
 'log_count/DEBUG': 66,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'request_depth_max': 1,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 61,
 'scheduler/dequeued/memory': 61,
 'scheduler/enqueued': 61,
 'scheduler/enqueued/memory': 61,
 'start_time': datetime.datetime(2018, 9, 19, 9, 18, 43, 100901)}
2018-09-19 17:18:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-19 17:25:18 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: example)
2018-09-19 17:25:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (default, Apr  1 2018, 05:46:30) - [GCC 7.3.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-34-generic-x86_64-with-Ubuntu-18.04-bionic
2018-09-19 17:25:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'example', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['example.spiders']}
2018-09-19 17:25:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-09-19 17:25:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-19 17:25:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-19 17:25:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-09-19 17:25:18 [scrapy.core.engine] INFO: Spider opened
2018-09-19 17:25:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-19 17:25:18 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-09-19 17:25:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:25:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:25:18 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:25:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2018-09-19 17:25:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/ubuntu/Scrapy/example/html/tm.html> (referer: None)
2018-09-19 17:25:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://detail.tmall.com/robots.txt> (referer: None)
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=10152256872&skuId=78140728882&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45716529622&skuId=3657827257584&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39235505054&skuId=3246644221942&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545020495706&skuId=3814932525943&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540210701968&skuId=3917892555229&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=36299592403&skuId=59473355147&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554738813806&skuId=3680965896205&standard=1&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959480906&skuId=3922173019832&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45669350384&skuId=3445712899582&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527922188250&skuId=3237788828996&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=40000831870&skuId=3911785195184&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=15354239784&skuId=3757349821066&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=550679604048&skuId=3950518295650&standard=1&user_id=3295616803&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=562830632869&skuId=3542491737156&user_id=2362132778&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221112652&skuId=3770833343471&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=544959928102&skuId=3812905236795&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570597116831&skuId=3851514038575&standard=1&user_id=704392951&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902338584&skuId=3789050113843&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540346037673&skuId=3796809784729&standard=1&user_id=2133729733&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523160102571&skuId=3909670414447&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523242229702&skuId=3910224794399&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=566803058725&skuId=3603270917229&user_id=713805252&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545194381711&skuId=3451976174898&standard=1&user_id=3089116113&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=541221612873&skuId=3811793005884&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570262192129&skuId=3676994888459&standard=1&user_id=2357345202&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569120030736&skuId=3730782856965&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554902686118&skuId=3773509711223&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538811534073&skuId=3916320122278&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=545775752075&skuId=3741617055871&standard=1&user_id=3162810140&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564063940519&skuId=3943960855846&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=565399873867&skuId=3753159932463&standard=1&user_id=925588507&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564240962668&skuId=3753511052366&standard=1&user_id=1577744489&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=540015263833&skuId=3929532822134&standard=1&user_id=2578636029&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45603735926&skuId=3850910643047&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569728176391&skuId=3831626550518&standard=1&user_id=2202597322&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=527893326581&skuId=3195559274344&standard=1&user_id=2731691808&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564570934197&skuId=3732859846129&standard=1&user_id=1124540496&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=12620021281&skuId=67511695221&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559863950838&skuId=3761114936423&standard=1&user_id=737581011&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564482793462&skuId=3566498168740&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=538776838968&skuId=3748932472633&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=561271454031&skuId=3713328932535&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=560178885692&skuId=3882569963923&standard=1&user_id=742795908&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=554824935356&skuId=3584276187036&user_id=3302094416&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=45053862121&skuId=3684716254711&standard=1&user_id=2067861716&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=570451573502&skuId=3755546720658&user_id=3817430018&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569129768864&skuId=3896327478114&standard=1&user_id=894127710&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=20351803834&skuId=3916397510844&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=25356196399&skuId=3804175157430&standard=1&user_id=695023374&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=536344697361&skuId=3421408557705&standard=1&user_id=2037117585&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=521322201495&skuId=3282961913302&standard=1&user_id=680956838&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564857606526&skuId=3572946977984&standard=1&user_id=2972174697&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=44198874506&skuId=3263893868155&standard=1&user_id=1705684430&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=17327350365&skuId=3916383698416&standard=1&user_id=874887241&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=553273646296&skuId=3396571852717&standard=1&user_id=2616970884&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=523307965803&skuId=3742872456901&standard=1&user_id=1746864514&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=39916555226&skuId=3520615749309&standard=1&user_id=1699764759&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=564100865607&skuId=3899592779357&standard=1&user_id=3208350802&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=569647996683&skuId=3916326182535&standard=1&user_id=2864394693&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://detail.tmall.com/item.htm?id=559258759150&skuId=3647414346979&standard=1&user_id=744506827&cat_id=2&is_b=1&rn=25791b57c83899cb6cd226f02961eb3d>
2018-09-19 17:25:19 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-19 17:25:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 63,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 60,
 'downloader/request_bytes': 1090,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 350661,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 19, 9, 25, 19, 277616),
 'log_count/DEBUG': 66,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 1076174848,
 'memusage/startup': 1076174848,
 'request_depth_max': 1,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 61,
 'scheduler/dequeued/memory': 61,
 'scheduler/enqueued': 61,
 'scheduler/enqueued/memory': 61,
 'start_time': datetime.datetime(2018, 9, 19, 9, 25, 18, 631838)}
2018-09-19 17:25:19 [scrapy.core.engine] INFO: Spider closed (finished)
