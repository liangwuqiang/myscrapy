<!DOCTYPE html>
        <html><head><meta charset="UTF-8">
        </head><body>
        <p><center><h1>scrapy代理IP的使用</h1></center></p>
            <div class="cont">
<h1>scrapy代理IP的使用</h1>
<p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">原理简述</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>原理可以简单的概述为:</p><p>用户(A)-在线代理服务器(B)-目标网站(C)，</p><p>即：A向B发送浏览请求-B执行请求发送给C-C收到请求，回应。</p><p><br/></p><p><img alt="Python1.png" src="images/4b067bd336fd57f074d9be4c6ecb0694.png" title="baFVhRnJgbH5iruwsm.png"/></p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">实现方法</span></h3><p><br/></p><p><strong>1, Spider Middlewares</strong></p><p><br/></p><p>Each middleware component is a Python class that defines one or more of the following methods:</p><p>class scrapy.spidermiddlewares.SpiderMiddleware</p><p> </p><p><strong>process_spider_input(response, spider)</strong></p><p> </p><p>This method is called for each response that goes through the spider middleware and into the spider, for processing.</p><p>process_spider_input() should return None or raise an exception.</p><p>If it returns None, Scrapy will continue processing this response, executing all other middlewares until, finally, the response is handed to the spider for processing.</p><p> </p><p>If it raises an exception, Scrapy won’t bother calling any other spider middleware process_spider_input() and will call the request errback. The output of the errback is chained back in the other direction for process_spider_output() to process it, or process_spider_exception() if it raised an exception.</p><p>Parameters </p><p>• response (Response object) – the response being processed </p><p>• spider (Spider object) – the spider for which this response is intended</p><p> </p><p><strong>process_spider_output(response, result, spider)</strong></p><p> </p><p>This method is called with the results returned from the Spider, after it has processed the response. </p><p>process_spider_output() must return an iterable of Request, dict or Item objects. </p><p>Parameters</p><p>• response (Response object) – the response which generated this output from the spi-der </p><p>• result (an iterable of Request, dict or Item objects) – the result returned by the spider </p><p>• spider (Spider object) – the spider whose result is being processed</p><p> </p><p><strong>process_spider_exception(response, exception, spider)</strong></p><p> </p><p>This method is called when when a spider or process_spider_input() method (from other spider middleware) raises an exception. </p><p>process_spider_exception() should return either None or an iterable of Response, dict or Item objects. </p><p>If it returns None, Scrapy will continue processing this exception, executing any other process_spider_exception() in the following middleware components, until no middleware components are left and the exception reaches the engine (where it’s logged and discarded). </p><p>If it returns an iterable the process_spider_output() pipeline kicks in, and no other process_spider_exception() will be called. </p><p>Parameters </p><p>• response (Response object) – the response being processed when the exception was raised </p><p>• exception (Exception object) – the exception raised </p><p>• spider (Spider object) – the spider which raised the exception</p><p><strong><br/></strong></p><p><strong>process_start_requests(start_requests, spider)</strong></p><p> </p><p>New in version 0.15.</p><p>This method is called with the start requests of the spider, and works similarly to the process_spider_output() method, except that it doesn’t have a response associated and must return only requests (not items). </p><p>It receives an iterable (in the start_requests parameter) and must return another iterable of Request objects.</p><p>Note: When implementing this method in your spider middleware, you should always return an iterable (that follows the input one) and not consume all start_requests iterator because it can be very large </p><p>(or even unbounded) and cause a memory overflow. The Scrapy engine is designed to pull start requests while it has capacity to process them, so the start requests iterator can be effectively endless where there is some other condition for stopping the spider (like a time limit or item/page count).</p><p>Parameters </p><p>• start_requests (an iterable of Request) – the start requests</p><p>•spider (Spider object) – the spider to whom the start requests belong</p><p> </p><p><strong>2, Downloader Middlewares</strong></p><p><strong><br/></strong></p><p>Each middleware component is a Python class that defines one or more of the following methods:</p><p>class scrapy.downloadermiddlewares.DownloaderMiddleware</p><p> </p><p><strong>process_request(request, spider)</strong></p><p> </p><p>This method is called for each request that goes through the download middleware. </p><p>process_request() should either: return None, return a Response object, return a Request object, or raise IgnoreRequest. </p><p>If it returns None, Scrapy will continue processing this request, executing all other middlewares until, finally, the appropriate downloader handler is called the request performed (and its response downloaded). </p><p>If it returns a Response object, Scrapy won’t bother calling any other process_request() or process_exception() methods, or the appropriate download function; it’ll return that response. The process_response() methods of installed middleware is always called on every response. </p><p>If it returns a Request object, Scrapy will stop calling process_request methods and reschedule the returned request. Once the newly returned request is performed, the appropriate middleware chain will be called on the downloaded response. </p><p>If it raises an IgnoreRequest exception, the process_exception() methods of installed down-loader middleware will be called. If none of them handle the exception, the errback function of the request (Request.errback) is called. If no code handles the raised exception, it is ignored and not logged (unlike other exceptions).</p><p> </p><p>Parameters </p><p>• request (Request object) – the request being processed </p><p>• spider (Spider object) – the spider for which this request is intended</p><p><br/></p><p><strong>process_response(request, response, spider)</strong></p><p> </p><p>process_response() should either: return a Response object, return a Request object or raise a </p><p>IgnoreRequest exception. </p><p>If it returns a Response (it could be the same given response, or a brand-new one), that response will continue to be processed with the process_response() of the next middleware in the chain. </p><p>If it returns a Request object, the middleware chain is halted and the returned request is resched-uled to be downloaded in the future. This is the same behavior as if a request is returned from process_request(). </p><p>If it raises an IgnoreRequest exception, the errback function of the request (Request.errback) is called. If no code handles the raised exception, it is ignored and not logged (unlike other exceptions).</p><p>Parameters </p><p>• request (is a Request object) – the request that originated the response </p><p>• response (Response object) – the response being processed </p><p>• spider (Spider object) – the spider for which this response is intended</p><p> </p><p><strong>process_exception(request, exception, spider)</strong></p><p> </p><p>Scrapy calls process_exception() when a download handler or a process_request() (from a downloader middleware) raises an exception (including an IgnoreRequest exception) </p><p>process_exception() should return: either None, a Response object, or a Request object.</p><p>If it returns None, Scrapy will continue processing this exception, executing any other process_exception() methods of installed middleware, until no middleware is left and the default exception handling kicks in. </p><p>If it returns a Response object, the process_response() method chain of installed middleware is started, and Scrapy won’t bother calling any other process_exception() methods of middleware.</p><p>If it returns a Request object, the returned request is rescheduled to be downloaded in the future. This stops the execution of process_exception() methods of the middleware the same as returning a response would.</p><p> </p><p>Parameters</p><p>request (is a Request object) – the request that generated the exception</p><p>exception (an Exception object) – the raised exception</p><p>spider (Spider object) – the spider for which this request is intended</p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">scrapy具体实现</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p><strong>1，代理IP的获取——西刺</strong></p><p><br/></p><p>这里使用我们之前讲过的tutorial，打开tutorial，打开middlewars，更改部分内容，更改部分的代码如图所示：</p><p><br/></p><p><img alt="Python1.png" src="images/849184822c286a53fb4274f5527932a2.png" title="B2BoHOYj1JkvDbOK4n.png"/> </p><p><strong><br/></strong></p><p><strong>2，代理IP的验证——socket</strong></p><p><strong><br/></strong></p><p><img alt="Python2.png" src="images/170a5c40cea4babae1cb269c95043bdc.png" title="AupFCpvOZCLxMbet83.png"/></p><p> </p><p><img alt="Python4.png" src="images/b02b392573837904f2904b00d0139204.png" title="A0ho2yq7G6G2WWjfnw.png"/></p><p><br/></p><p><img alt="Python5.png" src="images/a70617d61e3b09ba56368a5c6b92a828.png" title="6cQhpU9PSvsX3pmbrh.png"/> </p><p><br/></p><p><img alt="Python6.png" src="images/d4d08536e406d9c7bee87cfa6cff2fb5.png" title="lD2v2jlXKNTXIqIvoh.png"/> </p><p><br/></p><p><strong>在settings中启用：</strong></p><p><br/></p><p><img alt="Python7.png" src="images/b0858f6e3952a0438568db6ed4382d7c.png" title="gUrqLKtoUjF6wFcM26.png"/> </p><p><br/></p><p><strong>3，代理IP的使用——downloader middlewares</strong></p><p> </p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">总结</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>要求掌握：添加代理ip的方法</p><p><br/></p><p><br/></p><p><span style='color: rgb(192, 0, 0); font-family: "microsoft yahei"; font-weight: bold; line-height: 20px; background-color: rgb(255, 255, 255);'>【本文由麦子学院独家原创，转载请注明出处并保留原文链接】</span></p><p>
</p></p></div>
        </body></html>