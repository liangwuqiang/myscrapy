<!DOCTYPE html>
        <html><head><meta charset="UTF-8">
        </head><body>
        <p><center><h1>scrapy其他重要组件</h1></center></p>
            <div class="cont">
<h1>scrapy其他重要组件</h1>
<p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">items</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p><strong>一个items的完整示例</strong></p><p><strong><br/></strong></p><p>import scrapy</p><p> </p><p>class Product(scrapy.Item):</p><p>name = scrapy.Field()</p><p>price = scrapy.Field()</p><p>stock = scrapy.Field()</p><p>last_updated = scrapy.Field(serializer=str)</p><p> </p><p>Note: Those familiar with Django will notice that Scrapy Items are declared similar to Django Models, except that Scrapy Items are much simpler as there is no concept of different field types.</p><p> </p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">item pipeline</span><br/></h3><p><strong><span style="color: rgb(94, 207, 186);"><br/></span></strong></p><p><strong>1、Typical uses of item pipelines are:</strong></p><p><strong><br/></strong></p><p>• cleansing HTML data </p><p>• validating scraped data (checking that the items contain certain fields) </p><p>• checking for duplicates (and dropping them)</p><p>• storing the scraped item in a database</p><p><br/></p><p><strong>2、完整示例</strong></p><p><strong><br/></strong></p><p><strong>例1：</strong></p><p><strong><br/></strong></p><p>from scrapy.exceptions import DropItem</p><p>class PricePipeline(object):</p><p>vat_factor = 1.15</p><p>def process_item(self, item, spider): if item['price']:</p><p> </p><p>if item['price_excludes_vat']:</p><p> </p><p>item['price'] = item['price'] * self.vat_factor return item</p><p> </p><p>else:</p><p> </p><p>raise DropItem("Missing price in %s" % item)</p><p><br/></p><p><strong>例2：</strong></p><p><strong><br/></strong></p><p>Write items to a JSON file</p><p>The following pipeline stores all scraped items (from all spiders) into a a single items.jl file, containing one item per line serialized in JSON format:</p><p>import json</p><p>class JsonWriterPipeline(object):</p><p>def __init__(self):</p><p> </p><p>self.file = open('items.jl', 'wb')</p><p>def process_item(self, item, spider): line = json.dumps(dict(item)) + "\n" self.file.write(line)</p><p>return item</p><p><br/></p><p><strong>例3：</strong></p><p><strong><br/></strong></p><p>Duplicates filter</p><p>A filter that looks for duplicate items, and drops those items that were already processed. Let’s say that our items have a unique id, but our spider returns multiples items with the same id:</p><p>from scrapy.exceptions import DropItem</p><p> </p><p>class DuplicatesPipeline(object):</p><p>def __init__(self): self.ids_seen = set()</p><p>def process_item(self, item, spider): if item['id'] in self.ids_seen:</p><p> </p><p>raise DropItem("Duplicate item found: %s" % item)</p><p> </p><p>else:</p><p> </p><p>self.ids_seen.add(item['id']) return item</p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">Feed exports</span><br/></h3><p><strong><span style="color: rgb(94, 207, 186);"><br/></span></strong></p><p><strong>1、For serializing the scraped data, the feed exports use the Item exporters. These formats are supported out of the box:</strong></p><p> </p><p>• JSON</p><p>FEED_FORMAT: json</p><p>Exporter used: JsonItemExporter</p><p>See this warning if you’re using JSON with large feeds.</p><p> </p><p>• JSON lines</p><p>FEED_FORMAT: jsonlines</p><p>Exporter used: JsonLinesItemExporter</p><p> </p><p>• CSV</p><p>FEED_FORMAT: csv</p><p>Exporter used: CsvItemExporter</p><p>To specify columns to export and their order use FEED_EXPORT_FIELDS. Other feed exporters can also use this option, but it is important for CSV because unlike many other export formats CSV uses a fixed header.</p><p> </p><p>• XML</p><p>FEED_FORMAT: xml </p><p>Exporter used: XmlItemExporter</p><p> </p><p><strong>2、When using the feed exports you define where to store the feed using a URI (through the FEED_URI setting). The feed exports supports multiple storage backend types which are defined by the URI scheme.</strong></p><p> </p><p>The storages backends supported out of the box are:</p><p>• Local filesystem</p><p>• FTP</p><p>• S3 (requires boto)</p><p>• Standard output</p><p><br/></p><p><br/></p><p><span style='color: rgb(192, 0, 0); font-family: "microsoft yahei"; font-weight: bold; line-height: 20px; background-color: rgb(255, 255, 255);'>【本文由麦子学院独家原创，转载请注明出处并保留原文链接】</span></p><p>
</p></p></div>
        </body></html>