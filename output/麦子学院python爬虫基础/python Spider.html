<!DOCTYPE html>
        <html><head><meta charset="UTF-8">
        </head><body>
        <p><center><h1>python Spider</h1></center></p>
            <div class="cont">
<h1>python Spider</h1>
<p><p><br/></p><p><strong><br/></strong></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">基本介绍</span> <br/></h3><p><strong><br/></strong></p><p><strong>概念</strong></p><p><strong><br/></strong></p><p>spider是一个类，它定义了这样爬取一个网站，包括怎样跟踪连接、怎样抓取数据</p><p><br/></p><p><strong>循环执行流程</strong></p><p><strong><br/></strong></p><p>Generating the initial requests</p><p>Parse the response</p><p>Using selector</p><p>Store item</p><p><br/></p><p><strong>之前项目中的一个实例</strong></p><p><strong><br/></strong></p><p><img alt="Python1.png" src="images/035d6475475567b602c8f43100819670.png" title="mF6sI12TytD1fRmNM1.png"/> </p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">基类（scrapy.spider）</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>属性</p><p><br/></p><p><strong>·name：spider 的名称，要求唯一</strong></p><p><strong><br/></strong></p><p>A string which defines the name for this spider. The spider name is how the spider is located (and instan-tiated) by Scrapy, so it must be unique. However, nothing prevents you from instantiating more than one instance of the same spider. This is the most important spider attribute and it’s required.</p><p> </p><p>If the spider scrapes a single domain, a common practice is to name the spider after the domain, with or without the TLD. So, for example, a spider that crawls mywebsite.com would often be called mywebsite.</p><p><br/></p><p><strong>·allowed-domains：允许的域名</strong></p><p><strong><br/></strong></p><p>An optional list of strings containing domains that this spider is allowed to crawl. Requests for URLs not belonging to the domain names specified in this list won’t be followed if OffsiteMiddleware is enabled.</p><p><br/></p><p><strong>·start-urls：初始urls</strong></p><p><strong><br/></strong></p><p>A list of URLs where the spider will begin to crawl from, when no particular URLs are specified. So, the first pages downloaded will be those listed here. The subsequent URLs will be generated successively from data contained in the start URLs.</p><p><br/></p><p><strong>·Crawler：抓取器，spider将绑定到它上面</strong></p><p><strong><br/></strong></p><p>This attribute is set by the from_crawler() class method after initializating the class, and links to the Crawler object to which this spider instance is bound.</p><p> </p><p>Crawlers encapsulate a lot of components in the project for their single entry access (such as extensions, middlewares, signals managers, etc). See Crawler API to know more about them.</p><p>For a list of available built-in settings see: Built-in settings reference.</p><p><br/></p><p><strong>·Setting：配置实例，包含工程中所有的配置变量</strong></p><p><strong><br/></strong></p><p>Configuration on which this spider is been ran. This is a Settings instance, see the Settings topic for a detailed introduction on this subject.</p><p><br/></p><p><strong>·Logger：日志实例</strong></p><p><strong><br/></strong></p><p>Python logger created with the Spider’s name. You can use it to send log messages through it as described on Logging from Spiders.</p><p>方法</p><p><br/></p><p><strong>·from_crawler(crawler,*args,**kwargs)类方法，用于创建spider</strong></p><p><strong><br/></strong></p><p>This is the class method used by Scrapy to create your spiders.</p><p>You probably won’t need to override this directly, since the default implementation acts as a proxy to the __init__() method, calling it with the given arguments args and named arguments kwargs.</p><p>Nonetheless, this method sets the crawler and settings attributes in the new instance, so they can be accessed later inside the spider’s code.</p><p>Parameters</p><p><br/></p><p><strong>•crawler (Crawler instance) – crawler to which the spider will be bound</strong></p><p><strong><br/></strong></p><p><strong>•args (list) – arguments passed to the __init__() method</strong></p><p><strong><br/></strong></p><p><strong>•kwargs (dict) – keyword arguments passed to the __init__() method</strong></p><p><strong><br/></strong></p><p><strong>·start_requests():生成初始的requests</strong></p><p><strong><br/></strong></p><p>This method must return an iterable with the first Requests to crawl for this spider.</p><p>This is the method called by Scrapy when the spider is opened for scraping when no particular URLs are specified. If particular URLs are specified, the make_requests_from_url() is used instead to create the Requests. This method is also called only once from Scrapy, so it’s safe to implement it as a generator.</p><p>The default implementation uses make_requests_from_url() to generate Requests for each url in start_urls.</p><p>If you want to change the Requests used to start scraping a domain, this is the method to override. For example, if you need to start by logging in using a POST request, you could do:</p><p><br/></p><p><img alt="Python2.png" src="images/77ed3071baa4eaee1aa72556bdf2caf4.png" title="dWsAz1EQ7tMcMphAzf.png"/> </p><p> </p><p><strong>·make_requests_from_url(url)：根据url生成一个request</strong></p><p><strong><br/></strong></p><p>A method that receives a URL and returns a Request object (or a list of Request objects) to scrape. This method is used to construct the initial requests in the start_requests() method, and is typically used to convert urls to requests.</p><p>Unless overridden, this method returns Requests with the parse() method as their callback function, and with dont_filter parameter enabled (see Request class for more info).</p><p><br/></p><p><strong>·parse（response）用来解析网页内容</strong></p><p><strong><br/></strong></p><p>This is the default callback used by Scrapy to process downloaded responses, when their requests don’t specify a callback.</p><p>The parse method is in charge of processing the response and returning scraped data and/or more URLs to follow. Other Requests callbacks have the same requirements as the Spider class.</p><p>This method, as well as any other Request callback, must return an iterable of Request and/or dicts or Item objects.</p><p><br/></p><p><strong>·log（message[,level,component]）：用来记录日志，这里请使用logger属性记录日志。</strong></p><p><strong><br/></strong></p><p>Wrapper that sends a log message through the Spider’s logger, kept for backwards compatibility. For more information see Logging from Spiders.</p><p><br/></p><p><strong>·Self.logger.info(“visited success”)</strong></p><p><strong><br/></strong></p><p><strong>·Closed(reason):当spider关闭的时候调用的方法</strong></p><p><strong><br/></strong></p><p>Called when the spider closes. This method provides a shortcut to signals.connect() for the spider_closed signal.</p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">子类介绍(</span><span style="color: rgb(94, 207, 186);">Crawlspider)</span></h3><p><br/></p><p><strong>1、最常用的spider，用于抓取普通的网页</strong></p><p><strong><br/></strong></p><p><strong>2、增加了两个成员</strong></p><p><strong><br/></strong></p><p>  1）rules：定义了一些抓取规则——连接怎么跟踪、使用哪一个parse函数解析此链接；</p><p>Which is a list of one (or more) Rule objects. Each Rule defines a certain behaviour for crawling the site. Rules objects are described below. If multiple rules match the same link, the first one will be used, according to the order they’re defined in this attribute.</p><p><br/></p><p>  2）parse_start-url(response):解析初始的url的相应</p><p>This method is called for the start_urls responses. It allows to parse the initial responses and must return either an Item object, a Request object, or an iterable containing any of them.</p><p><span style='font-weight: 700; margin: 0px; padding: 0px; border: 0px; outline: 0px; color: rgb(51, 51, 51); font-family: "microsoft yahei"; line-height: 20px; background: rgb(255, 255, 255);'><span style="margin: 0px; padding: 0px; border: 0px; outline: 0px; color: rgb(192, 0, 0); background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;"><br/></span></span></p><p><span style='font-weight: 700; margin: 0px; padding: 0px; border: 0px; outline: 0px; color: rgb(51, 51, 51); font-family: "microsoft yahei"; line-height: 20px; background: rgb(255, 255, 255);'><span style="margin: 0px; padding: 0px; border: 0px; outline: 0px; color: rgb(192, 0, 0); background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;"><br/></span></span></p><p><span style='font-weight: 700; margin: 0px; padding: 0px; border: 0px; outline: 0px; color: rgb(51, 51, 51); font-family: "microsoft yahei"; line-height: 20px; background: rgb(255, 255, 255);'><span style="margin: 0px; padding: 0px; border: 0px; outline: 0px; color: rgb(192, 0, 0); background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">【本文由麦子学院独家原创，转载请注明出处并保留原文链接】</span></span></p><p>
</p></p></div>
        </body></html>