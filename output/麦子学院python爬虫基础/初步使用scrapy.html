<!DOCTYPE html>
        <html><head><meta charset="UTF-8">
        </head><body>
        <p><center><h1>初步使用scrapy</h1></center></p>
            <div class="cont">
<h1>初步使用scrapy</h1>
<p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">源代码</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p><img alt="Python1.png" src="images/3d8609c7fbac53889e01b1268a257870.png" title="MBo3YH0lh2x99qNGFR.png"/> </p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">原理讲解</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>1、在somefile.py文件中找到已定义的爬虫，然后通过抓取引擎运行爬虫。</p><p><br/></p><p>2、具体的抓取过程：</p><p><br/></p><p style="text-indent: 2em;">1） 使用start_urls作为初始url生成Request，并默认把parse作为它的回调函数。</p><p style="text-indent: 2em;">2）在parse中采用css选择器获得目标URL，并注册parse_question作为目标URL的回调函数。</p><p style="text-indent: 2em;"><br/></p><p>3.注：</p><p>1，请求被异步的调度、处理。</p><p>2，有一些参数可以控制过程，比如每个域名/ip的并发请求数、请求之间的下载延迟（或者自动调节）</p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">高级特性</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>1、内置的数据抽取器</p><p>2、交互式控制台用于调试数据抽取方法</p><p>3、内置对结果输出的支持，可以保存为JSON, CSV, XML等</p><p>4、自动处理编码</p><p>5、支持自定义扩展</p><p>6、丰富的内置扩展，可用于处理：</p><p><br/></p><p style="text-indent: 2em;">1）cookies and session（记录客户属性，如用户名、密码）</p><p style="text-indent: 2em;">2）HTTP features like compression, authentication, caching</p><p style="text-indent: 2em;">3） user-agent spoofing</p><p style="text-indent: 2em;">4）robots.txt</p><p style="text-indent: 2em;">5） crawl depth restriction</p><p style="text-indent: 2em;"><br/></p><p>7、远程调试scrapy</p><p>8、更多的支持，比如可爬取xml、csv，可自动下载图片等等。</p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">总结</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>1、掌握简单爬虫的编写方法，如本节示例；</p><p>2、了解scrapy的更多特性；</p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">作业</span></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;">模仿本节所讲的爬虫示例，实现“联合早报网”的“全球金融观察”页面列表中新闻（暂忽略翻页）的抓取，并把抓取的内容保存为csv</p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;"><br/></p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;"><strong>要求：</strong></p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;"><strong><br/></strong></p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;">初始URL：http：//www.zaobao.com/special/report/politic/fincrisis</p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;">内容页抓取：标题（title）、时间（dt）、正文（body）、</p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;">链接（link）</p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;">数据保存格式：csv</p><p style="margin-top: 0px; margin-bottom: 0px; padding: 0px; white-space: normal;">可选附加：保存屏幕的输出日志到文件（scrapy.log）中</p><p><br/></p><p><br/></p><p><span style='color: rgb(192, 0, 0); font-family: "microsoft yahei"; font-weight: bold; line-height: 20px; background-color: rgb(255, 255, 255);'>【本文由麦子学院独家原创，转载请注明出处并保留原文链接】</span></p><p>
</p></p></div>
        </body></html>