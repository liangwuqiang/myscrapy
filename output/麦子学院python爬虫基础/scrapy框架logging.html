<!DOCTYPE html>
        <html><head><meta charset="UTF-8">
        </head><body>
        <p><center><h1>scrapy框架logging</h1></center></p>
            <div class="cont">
<h1>scrapy框架logging</h1>
<p><p><br/></p><p>scrapy使用python内置的logging模块记录日志</p><p> </p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">日志的级别</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>1. logging.CRITICAL - for critical errors (highest severity)</p><p>2. logging.ERROR - for regular errors</p><p>3. logging.WARNING - for warning messages</p><p>4. logging.INFO - for informational messages</p><p>5. logging.DEBUG - for debugging messages (lowest severity)</p><p><br/></p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">基本使用方法</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p><strong>1.简单使用方法</strong></p><p><strong><br/></strong></p><p>import logging</p><p>Logging.warning(“this is a test ”)</p><p>执行结果：</p><p><br/></p><p><img alt="Python1.png" src="images/23def513d7d531e5a75a978563fbc4f6.png" title="jgiLrRknVOgxI0LTK5.png"/> </p><p><br/></p><p><strong>2.通用的记录日志的方法，可加入日志的级别</strong></p><p><strong><br/></strong></p><p>import logging</p><p>Logging.log(logging.WARNING,”this is a warning”)</p><p><br/></p><p><strong>3，通过logger记录日志</strong></p><p><strong><br/></strong></p><p>import logging</p><p>logger=logging.getLogger(_name_)</p><p>Logger.warning(“this is a warning”)</p><p> </p><h3><span style="color: rgb(94, 207, 186);"></span></h3><hr/><h3><span style="color: rgb(94, 207, 186);">在scrapy中使用</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>Scrapy provides a logger within each Spider instance, that can be accessed and used like this:</p><p>import scrapy</p><p> </p><p><strong>class MySpider(scrapy.Spider):</strong></p><p> </p><p>name = 'myspider'</p><p>start_urls = ['http://scrapinghub.com']</p><p> </p><p><strong>def parse(self, response):</strong></p><p> </p><p>self.logger.info('Parse function called on %s', response.url)</p><p>That logger is created using the Spider’s name, but you can use any custom Python logger you want. For example:</p><p>import logging import scrapy</p><p>logger = logging.getLogger('mycustomlogger')</p><p> </p><p><strong>class MySpider(scrapy.Spider):</strong></p><p> </p><p>name = 'myspider'</p><p>start_urls = ['http://scrapinghub.com']</p><p><br/></p><p><strong>def parse(self, response):</strong></p><p> </p><p>logger.info('Parse function called on %s', response.url)</p><p><br/></p><p></p><hr/><p></p><h3><span style="color: rgb(94, 207, 186);">在settings.py中配置</span><br/></h3><p><span style="color: rgb(94, 207, 186);"><br/></span></p><p>These settings can be used to configure the logging:</p><p> </p><p>• LOG_FILE </p><p>• LOG_ENABLED</p><p>• LOG_ENCODING</p><p>• LOG_LEVEL </p><p>• LOG_FORMAT </p><p>• LOG_DATEFORMAT </p><p>• LOG_STDOUT</p><p><br/></p><p><br/></p><p><span style='color: rgb(192, 0, 0); font-family: "microsoft yahei"; font-weight: bold; line-height: 20px; background-color: rgb(255, 255, 255);'>【本文由麦子学院独家原创，转载请注明出处并保留原文链接】</span></p><p>
</p></p></div>
        </body></html>