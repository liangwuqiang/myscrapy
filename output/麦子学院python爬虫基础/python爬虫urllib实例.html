<!DOCTYPE html>
        <html><head><meta charset="UTF-8">
        </head><body>
        <p><center><h1>python爬虫urllib实例</h1></center></p>
            <div class="cont">
<h1>python爬虫urllib实例</h1>
<p><p><br/></p><hr/><h3><span style="color: rgb(94, 207, 186);">简单的爬虫实例：从雅虎财经获取股票数据</span></h3><p><br/></p><p>雅虎财经股票数据接口介绍</p><p><br/></p><p><strong>1）股票数据</strong></p><p><br/></p><p><img alt="case1.png" src="images/93b5793f8f515764814f6bae138534f3.png" title="yi89h85HCeNRw0SfHA.png"/></p><p><br/></p><p><strong>2）时间参数</strong></p><p><br/></p><p><img alt="case2.png" src="images/ae8dc93f4a48f320cc424ed5b133e73f.png" title="Qqqp0ixZCYS66CpR69.png"/></p><p><br/></p><p>A，b，c表示开始的时间，d，e，f，表示结束的时间，s表示源代码，这个的月份是0，因为月份的接口是从0开始的。</p><p>我们来看一下深市数据，这个就是完整的股票数据，这个数据的格式是日交易数据，其中包含时间，开盘价，最高价，最低价，收盘价，成交量，还有逐群以后的价格。</p><p><br/></p><p><img alt="case3.png" height="514" src="images/fddd6dbb64586c9b9a904615811576c2.png" style="width: 866px; height: 514px;" title="kU9zuxHVN8OiMxRRqQ.png" width="866"/></p><p><br/></p><p>我们再来看一下带时间参数的。</p><p><br/></p><p><img alt="case4.png" height="482" src="images/5290492927e8a204d55db7beb84ee6ea.png" style="width: 862px; height: 482px;" title="ECKZVpj9kNYC7zNOUz.png" width="862"/></p><p><br/></p><p>我们如何在代码里面去实现把这个股票数据给它爬下来？</p><p><br/></p><p>示例：</p><p># -*- coding: utf-8 -*-</p><p>import urllib</p><p>import datetime</p><p><br/></p><p>def download_stock_data(stock_list):  //下载数据的函数，参数就是股票的列表</p><p>    for sid in stock_list:</p><p>        url = 'http://table.finance.yahoo.com/table.csv?s=' + sid</p><p>        fname = sid + '.csv'  //文件名为股票的id加上后缀</p><p>        urllib.urlretrieve(url, fname)</p><p><br/></p><p>if __name__ == '__main__':</p><p>    stock_list = ['300001.sz', '300002.sz']   //下载前两个股票</p><p>download_stock_data(stock_list)</p><p><br/></p><p>运行结果</p><p><br/></p><p><img alt="case5.png" src="images/752bb8f08be250a9d659a4c87e2fcc43.png" title="0UyI5nFjNItmkjWGk7.png"/></p><p><br/></p><p>可以看到运行完了，这里面多了两个文件出来，打开看一下。</p><p><br/></p><p><img alt="case6.png" src="images/50be162b912ec9ce7b3b82890c9317b8.png" title="msYhqfeEe5Q5JffcjS.png"/></p><p><br/></p><p><img alt="case7.png" height="419" src="images/8f01f5c579c446978e7e5ccaa91d301b.png" style="width: 829px; height: 419px;" title="kkfMA1QqR4Tml0mTL0.png" width="829"/></p><p><br/></p><p>修改代码，让过程看的更清楚一点</p><p><br/></p><p># -*- coding: utf-8 -*-</p><p>import urllib</p><p>import datetime</p><p><br/></p><p>def download_stock_data(stock_list):  //下载数据的函数，参数就是股票的列表</p><p>    for sid in stock_list:</p><p>        url = 'http://table.finance.yahoo.com/table.csv?s=' + sid</p><p>        fname = sid + '.csv'  //文件名为股票的id加上后缀</p><p>        print('downloading %s form %s' % (fname, url))  //把网址和文件名都打印出来</p><p>        urllib.urlretrieve(url, fname)</p><p><br/></p><p>if __name__ == '__main__':</p><p>    stock_list = ['300001.sz', '300002.sz']   //下载前两个股票</p><p>download_stock_data(stock_list)</p><p><br/></p><p>运行结果</p><p><br/></p><p><img alt="case8.png" src="images/3487458400b3369e8f7082327952ed7d.png" title="T080YjD1RVpjyTVG7I.png"/></p><p><br/></p><p>接下来，我们来实现另一个函数，就是从雅虎的财经网站上去下载指定时间段的股票的日交易数据，代码如下：</p><p><br/></p><p># -*- coding: utf-8 -*-</p><p>import urllib</p><p>import datetime</p><p><br/></p><p>def download_stock_data_in_period(stock_list, start, end): //指定时间段，参数为股票的列表，一个为开始时间，一个为结束时间</p><p>    for sid in stock_list:</p><p>        params = {'a': start.month - 1, 'b': start.day, 'c': start.year,</p><p>                  'd': end.month - 1, 'e': end.day, 'f': end.year, 's': sid} //a开始时间月份减1，b开始时间日，c开始时间年，d结束时间月份减1，e结束时间日，f结束时间年，还有一个是股票的id</p><p>        url = 'http://table.finance.yahoo.com/table.csv?'</p><p>        qs = urllib.urlencode(params)</p><p>        url = url + qs</p><p>        fname = '%s_%d%d%d_%d%d%d.csv' % (sid, start.year, start.month, start.day,</p><p>                                          end.year, end.month, end.day) </p><p>//%s_%d%d%d_%d%d%d起始日期加上一个结束日期</p><p>        print('downloading %s from %s' % (fname, url)) //调试，打印出文件名和url</p><p>        urllib.urlretrieve(url, fname)</p><p>       </p><p>if __name__ == '__main__':</p><p>    stock_list = ['300001.sz', '300002.sz']</p><p>    end = datetime.date(year=2015, month=12, day=17)  //开始时间</p><p>    start = datetime.date(year=2015, month=11, day=17)   //结束时间</p><p>download_stock_data_in_period(stock_list, start, end)  //下载区间数据，从2015年11月份到12月份的数据</p><p><br/></p><p>运行结果，可以看到，我们的url是这样复杂的数据</p><p><br/></p><p><img alt="case9.png" height="82" src="images/30c51980342b0b3b4dda15ca617a1b02.png" style="width: 986px; height: 82px;" title="WYohqFCk45iYOIE9jW.png" width="986"/></p><p><br/></p><p>下载数据</p><p><br/></p><p><img alt="case10.png" src="images/e43069b2836574bf239e24f9b4683d8d.png" title="Rv9Hd6iqCGw7ggOalH.png"/></p><p><br/></p><p><img alt="case11.png" height="343" src="images/868b988e020b25e56bfd1e4d678b85d1.png" style="width: 878px; height: 343px;" title="RBndB953AX4U2K1KZG.png" width="878"/></p><p><br/></p><p>我们用这个区下载，如果遇到这个股票不存在的时候怎么办呢？</p><p><br/></p><p>比如</p><p><br/></p><p># -*- coding: utf-8 -*-</p><p>import urllib</p><p>import datetime</p><p><br/></p><p>def download_stock_data_in_period(stock_list, start, end): //指定时间段，参数为股票的列表，一个为开始时间，一个为结束时间</p><p>    for sid in stock_list:</p><p>        params = {'a': start.month - 1, 'b': start.day, 'c': start.year,</p><p>                  'd': end.month - 1, 'e': end.day, 'f': end.year, 's': sid} //a开始时间月份减1，b开始时间日，c开始时间年，d结束时间月份减1，e结束时间日，f结束时间年，还有一个是股票的id</p><p>        url = 'http://table.finance.yahoo.com/table.csv?'</p><p>        qs = urllib.urlencode(params)</p><p>        url = url + qs</p><p>        fname = '%s_%d%d%d_%d%d%d.csv' % (sid, start.year, start.month, start.day,</p><p>                                          end.year, end.month, end.day) </p><p>//%s_%d%d%d_%d%d%d起始日期加上一个结束日期</p><p>        print('downloading %s from %s' % (fname, url)) //调试，打印出文件名和url</p><p>        urllib.urlretrieve(url, fname)</p><p><br/></p><p>if __name__ == '__main__':</p><p>    stock_list = ['300001.sz', '310002.sz'] //310002这是个不存在的数据</p><p>    end = datetime.date(year=2015, month=12, day=17)  //开始时间</p><p>    start = datetime.date(year=2015, month=11, day=17)   //结束时间</p><p>download_stock_data_in_period(stock_list, start, end)  //下载区间数据，从2015年11月份到12月份的数据</p><p><br/></p><p>运行结果，看起来是成功了</p><p><br/></p><p><img alt="case12.png" height="73" src="images/768accc31c88b088c5a074eb96b806a2.png" style="width: 949px; height: 73px;" title="AM1PzSktfvFzWe1FK9.png" width="949"/></p><p><br/></p><p>但是，我们看数据，返回了一个404的错误，也就是，如果这个代码不存在的话，我们是解析不出来的</p><p><br/></p><p><img alt="case13.png" height="331" src="images/b6d5c93a6830d53754189a3722bd732e.png" style="width: 949px; height: 331px;" title="lm4JgnHM3aeqY0KmY6.png" width="949"/></p><p><br/></p><p>回忆一下，我们之前有介绍用urlopen可以解觉这个404的错误，能够进行一些处理，代码如下：</p><p><br/></p><p># -*- coding: utf-8 -*-</p><p>import urllib</p><p>import datetime</p><p><br/></p><p>def download_stock_data_in_period(stock_list, start, end): //指定时间段，参数为股票的列表，一个为开始时间，一个为结束时间</p><p>    for sid in stock_list:</p><p>        params = {'a': start.month - 1, 'b': start.day, 'c': start.year,</p><p>                  'd': end.month - 1, 'e': end.day, 'f': end.year, 's': sid} //a开始时间月份减1，b开始时间日，c开始时间年，d结束时间月份减1，e结束时间日，f结束时间年，还有一个是股票的id</p><p>        url = 'http://table.finance.yahoo.com/table.csv?'</p><p>        qs = urllib.urlencode(params)</p><p>        url = url + qs</p><p>        fname = '%s_%d%d%d_%d%d%d.csv' % (sid, start.year, start.month, start.day,</p><p>                                          end.year, end.month, end.day) </p><p>//%s_%d%d%d_%d%d%d起始日期加上一个结束日期</p><p>        print('downloading %s from %s' % (fname, url)) //调试，打印出文件名和url</p><p>        urllib.urlretrieve(url, fname)</p><p>        urllib.urlopen()</p><p><br/></p><p>if __name__ == '__main__':</p><p>    stock_list = ['300001.sz', '310002.sz'] //310002这是个不存在的数据</p><p>    end = datetime.date(year=2015, month=12, day=17)  //开始时间</p><p>    start = datetime.date(year=2015, month=11, day=17)   //结束时间</p><p>download_stock_data_in_period(stock_list, start, end)  //下载区间数据，从2015年11月份到12月份的数据</p><p><br/></p><p><br/></p><p><br/></p><p><strong><span style="color: rgb(192, 0, 0);">【本文由麦子学院独家原创，转载请注明出处并保留原文链接】</span></strong></p><p>
</p></p></div>
        </body></html>