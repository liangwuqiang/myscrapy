            try:
                file_urls = div.xpath('div[@class="productImg-wrap"]/a[1]/img/@src|'
                'div[@class="productImg-wrap"]/a[1]/img/@data-ks-lazyload').extract()[0]
                # item['file_urls'] = "http:"+file_urls if "//" in file_urls else "http://" + file_urls
                item['file_urls'] = ["http:" + file_urls]

这里需要的是一个列表
---------------------------------------------------
            yield scrapy.Request(url=item["GOODS_URL"], callback=self.parse_detail)
            # print(item["GOODS_URL"])

    def parse_detail(self, response):

        # div = response.xpath('//div[@class="extend"]/ul')
        # print(response.url)
        print('结束=============================================================')

为啥无法回调
解决 使用scrapy爬取淘宝页面的时候，在提交http请求时出现debug信息Forbidden by robots.txt，看来是请求被拒绝了。
默认scrapy遵守robot协议
在setting改变ROBOTSTXT_OBEY为False，让scrapy不要遵守robot协议，之后就能正常爬取了
---------------------------------------------------
response的类型：
<class 'scrapy.http.response.html.HtmlResponse'>

ip_list = response.xpath('//table[@id="ip_list"]')的类型
<class 'scrapy.selector.unified.Selector'>

------------------------------------------------------------------------

html = response.xpath("/html/body")
tr = html.xpath(".//tr") #搜索body下的所有tr必须加上'.', 否则搜索的是整个文档的所有tr

直接使用 tr = html.xpath("tr")  无响应 不知道为啥不行？
----------------------------------------------

cmdline.execute("scrapy crawl xici -o xici.csv".split())
return items   可以有csv输出
yield items  没有输出
----------------------------------------------

西刺中
print(response.body.decode('utf-8'))  可行
天猫中
print(response.body.decode('gbk')) 不行  无法解码
body本身返回的是byte类型，这里跳过吧
